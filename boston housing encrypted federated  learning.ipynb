{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0820 23:21:33.032045  5108 secure_random.py:26] Falling back to insecure randomness since the required custom op could not be found for the installed version of TensorFlow. Fix this by compiling custom ops. Missing file was 'C:\\Users\\rohit\\Anaconda3\\envs\\pysyft\\lib\\site-packages\\tf_encrypted/operations/secure_random/secure_random_module_tf_1.14.0.so'\n",
      "W0820 23:21:33.312295  5108 deprecation_wrapper.py:119] From C:\\Users\\rohit\\Anaconda3\\envs\\pysyft\\lib\\site-packages\\tf_encrypted\\session.py:26: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import syft as sy\n",
    "import torch as th\n",
    "hook = sy.TorchHook(th)\n",
    "from torch import nn, optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0820 23:21:55.718408  5108 base.py:646] Worker me already exists. Replacing old worker which could cause                     unexpected behavior\n",
      "W0820 23:21:55.720404  5108 base.py:646] Worker me already exists. Replacing old worker which could cause                     unexpected behavior\n",
      "W0820 23:21:55.721402  5108 base.py:646] Worker me already exists. Replacing old worker which could cause                     unexpected behavior\n"
     ]
    }
   ],
   "source": [
    "bob = sy.VirtualWorker(hook, id=\"bob\").add_worker(sy.local_worker)\n",
    "alice = sy.VirtualWorker(hook, id=\"alice\").add_worker(sy.local_worker)\n",
    "secure_worker = sy.VirtualWorker(hook, id=\"secure_worker\").add_worker(sy.local_worker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from sklearn.datasets import load_boston\n",
    "\n",
    "boston = load_boston()\n",
    "\n",
    "# A Toy Dataset\n",
    "data = th.tensor(boston.data, requires_grad=True)\n",
    "target = th.tensor(boston.target, requires_grad=True)\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(2, 20)\n",
    "        self.fc2 = nn.Linear(20, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# A Toy Model\n",
    "model = Net()\n",
    "\n",
    "def train():\n",
    "    # Training Logic\n",
    "    opt = optim.SGD(params=model.parameters(),lr=0.1)\n",
    "    for iter in range(20):\n",
    "\n",
    "        # 1) erase previous gradients (if they exist)\n",
    "        opt.zero_grad()\n",
    "\n",
    "        # 2) make a prediction\n",
    "        pred = model(data)\n",
    "\n",
    "        # 3) calculate how much we missed\n",
    "        loss = ((pred - target)**2).sum()\n",
    "\n",
    "        # 4) figure out which weights caused us to miss\n",
    "        loss.backward()\n",
    "\n",
    "        # 5) change those weights\n",
    "        opt.step()\n",
    "\n",
    "        # 6) print our progress\n",
    "        print(loss.data)\n",
    "        \n",
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type of boston dataset: <class 'sklearn.utils.Bunch'>\n"
     ]
    }
   ],
   "source": [
    "#From sklearn tutorial.\n",
    "from sklearn.datasets import load_boston\n",
    "boston = load_boston()\n",
    "print( \"Type of boston dataset:\", type(boston))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'data': array([[6.3200e-03, 1.8000e+01, 2.3100e+00, ..., 1.5300e+01, 3.9690e+02,\n",
       "         4.9800e+00],\n",
       "        [2.7310e-02, 0.0000e+00, 7.0700e+00, ..., 1.7800e+01, 3.9690e+02,\n",
       "         9.1400e+00],\n",
       "        [2.7290e-02, 0.0000e+00, 7.0700e+00, ..., 1.7800e+01, 3.9283e+02,\n",
       "         4.0300e+00],\n",
       "        ...,\n",
       "        [6.0760e-02, 0.0000e+00, 1.1930e+01, ..., 2.1000e+01, 3.9690e+02,\n",
       "         5.6400e+00],\n",
       "        [1.0959e-01, 0.0000e+00, 1.1930e+01, ..., 2.1000e+01, 3.9345e+02,\n",
       "         6.4800e+00],\n",
       "        [4.7410e-02, 0.0000e+00, 1.1930e+01, ..., 2.1000e+01, 3.9690e+02,\n",
       "         7.8800e+00]]),\n",
       " 'target': array([24. , 21.6, 34.7, 33.4, 36.2, 28.7, 22.9, 27.1, 16.5, 18.9, 15. ,\n",
       "        18.9, 21.7, 20.4, 18.2, 19.9, 23.1, 17.5, 20.2, 18.2, 13.6, 19.6,\n",
       "        15.2, 14.5, 15.6, 13.9, 16.6, 14.8, 18.4, 21. , 12.7, 14.5, 13.2,\n",
       "        13.1, 13.5, 18.9, 20. , 21. , 24.7, 30.8, 34.9, 26.6, 25.3, 24.7,\n",
       "        21.2, 19.3, 20. , 16.6, 14.4, 19.4, 19.7, 20.5, 25. , 23.4, 18.9,\n",
       "        35.4, 24.7, 31.6, 23.3, 19.6, 18.7, 16. , 22.2, 25. , 33. , 23.5,\n",
       "        19.4, 22. , 17.4, 20.9, 24.2, 21.7, 22.8, 23.4, 24.1, 21.4, 20. ,\n",
       "        20.8, 21.2, 20.3, 28. , 23.9, 24.8, 22.9, 23.9, 26.6, 22.5, 22.2,\n",
       "        23.6, 28.7, 22.6, 22. , 22.9, 25. , 20.6, 28.4, 21.4, 38.7, 43.8,\n",
       "        33.2, 27.5, 26.5, 18.6, 19.3, 20.1, 19.5, 19.5, 20.4, 19.8, 19.4,\n",
       "        21.7, 22.8, 18.8, 18.7, 18.5, 18.3, 21.2, 19.2, 20.4, 19.3, 22. ,\n",
       "        20.3, 20.5, 17.3, 18.8, 21.4, 15.7, 16.2, 18. , 14.3, 19.2, 19.6,\n",
       "        23. , 18.4, 15.6, 18.1, 17.4, 17.1, 13.3, 17.8, 14. , 14.4, 13.4,\n",
       "        15.6, 11.8, 13.8, 15.6, 14.6, 17.8, 15.4, 21.5, 19.6, 15.3, 19.4,\n",
       "        17. , 15.6, 13.1, 41.3, 24.3, 23.3, 27. , 50. , 50. , 50. , 22.7,\n",
       "        25. , 50. , 23.8, 23.8, 22.3, 17.4, 19.1, 23.1, 23.6, 22.6, 29.4,\n",
       "        23.2, 24.6, 29.9, 37.2, 39.8, 36.2, 37.9, 32.5, 26.4, 29.6, 50. ,\n",
       "        32. , 29.8, 34.9, 37. , 30.5, 36.4, 31.1, 29.1, 50. , 33.3, 30.3,\n",
       "        34.6, 34.9, 32.9, 24.1, 42.3, 48.5, 50. , 22.6, 24.4, 22.5, 24.4,\n",
       "        20. , 21.7, 19.3, 22.4, 28.1, 23.7, 25. , 23.3, 28.7, 21.5, 23. ,\n",
       "        26.7, 21.7, 27.5, 30.1, 44.8, 50. , 37.6, 31.6, 46.7, 31.5, 24.3,\n",
       "        31.7, 41.7, 48.3, 29. , 24. , 25.1, 31.5, 23.7, 23.3, 22. , 20.1,\n",
       "        22.2, 23.7, 17.6, 18.5, 24.3, 20.5, 24.5, 26.2, 24.4, 24.8, 29.6,\n",
       "        42.8, 21.9, 20.9, 44. , 50. , 36. , 30.1, 33.8, 43.1, 48.8, 31. ,\n",
       "        36.5, 22.8, 30.7, 50. , 43.5, 20.7, 21.1, 25.2, 24.4, 35.2, 32.4,\n",
       "        32. , 33.2, 33.1, 29.1, 35.1, 45.4, 35.4, 46. , 50. , 32.2, 22. ,\n",
       "        20.1, 23.2, 22.3, 24.8, 28.5, 37.3, 27.9, 23.9, 21.7, 28.6, 27.1,\n",
       "        20.3, 22.5, 29. , 24.8, 22. , 26.4, 33.1, 36.1, 28.4, 33.4, 28.2,\n",
       "        22.8, 20.3, 16.1, 22.1, 19.4, 21.6, 23.8, 16.2, 17.8, 19.8, 23.1,\n",
       "        21. , 23.8, 23.1, 20.4, 18.5, 25. , 24.6, 23. , 22.2, 19.3, 22.6,\n",
       "        19.8, 17.1, 19.4, 22.2, 20.7, 21.1, 19.5, 18.5, 20.6, 19. , 18.7,\n",
       "        32.7, 16.5, 23.9, 31.2, 17.5, 17.2, 23.1, 24.5, 26.6, 22.9, 24.1,\n",
       "        18.6, 30.1, 18.2, 20.6, 17.8, 21.7, 22.7, 22.6, 25. , 19.9, 20.8,\n",
       "        16.8, 21.9, 27.5, 21.9, 23.1, 50. , 50. , 50. , 50. , 50. , 13.8,\n",
       "        13.8, 15. , 13.9, 13.3, 13.1, 10.2, 10.4, 10.9, 11.3, 12.3,  8.8,\n",
       "         7.2, 10.5,  7.4, 10.2, 11.5, 15.1, 23.2,  9.7, 13.8, 12.7, 13.1,\n",
       "        12.5,  8.5,  5. ,  6.3,  5.6,  7.2, 12.1,  8.3,  8.5,  5. , 11.9,\n",
       "        27.9, 17.2, 27.5, 15. , 17.2, 17.9, 16.3,  7. ,  7.2,  7.5, 10.4,\n",
       "         8.8,  8.4, 16.7, 14.2, 20.8, 13.4, 11.7,  8.3, 10.2, 10.9, 11. ,\n",
       "         9.5, 14.5, 14.1, 16.1, 14.3, 11.7, 13.4,  9.6,  8.7,  8.4, 12.8,\n",
       "        10.5, 17.1, 18.4, 15.4, 10.8, 11.8, 14.9, 12.6, 14.1, 13. , 13.4,\n",
       "        15.2, 16.1, 17.8, 14.9, 14.1, 12.7, 13.5, 14.9, 20. , 16.4, 17.7,\n",
       "        19.5, 20.2, 21.4, 19.9, 19. , 19.1, 19.1, 20.1, 19.9, 19.6, 23.2,\n",
       "        29.8, 13.8, 13.3, 16.7, 12. , 14.6, 21.4, 23. , 23.7, 25. , 21.8,\n",
       "        20.6, 21.2, 19.1, 20.6, 15.2,  7. ,  8.1, 13.6, 20.1, 21.8, 24.5,\n",
       "        23.1, 19.7, 18.3, 21.2, 17.5, 16.8, 22.4, 20.6, 23.9, 22. , 11.9]),\n",
       " 'feature_names': array(['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD',\n",
       "        'TAX', 'PTRATIO', 'B', 'LSTAT'], dtype='<U7'),\n",
       " 'DESCR': \".. _boston_dataset:\\n\\nBoston house prices dataset\\n---------------------------\\n\\n**Data Set Characteristics:**  \\n\\n    :Number of Instances: 506 \\n\\n    :Number of Attributes: 13 numeric/categorical predictive. Median Value (attribute 14) is usually the target.\\n\\n    :Attribute Information (in order):\\n        - CRIM     per capita crime rate by town\\n        - ZN       proportion of residential land zoned for lots over 25,000 sq.ft.\\n        - INDUS    proportion of non-retail business acres per town\\n        - CHAS     Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)\\n        - NOX      nitric oxides concentration (parts per 10 million)\\n        - RM       average number of rooms per dwelling\\n        - AGE      proportion of owner-occupied units built prior to 1940\\n        - DIS      weighted distances to five Boston employment centres\\n        - RAD      index of accessibility to radial highways\\n        - TAX      full-value property-tax rate per $10,000\\n        - PTRATIO  pupil-teacher ratio by town\\n        - B        1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town\\n        - LSTAT    % lower status of the population\\n        - MEDV     Median value of owner-occupied homes in $1000's\\n\\n    :Missing Attribute Values: None\\n\\n    :Creator: Harrison, D. and Rubinfeld, D.L.\\n\\nThis is a copy of UCI ML housing dataset.\\nhttps://archive.ics.uci.edu/ml/machine-learning-databases/housing/\\n\\n\\nThis dataset was taken from the StatLib library which is maintained at Carnegie Mellon University.\\n\\nThe Boston house-price data of Harrison, D. and Rubinfeld, D.L. 'Hedonic\\nprices and the demand for clean air', J. Environ. Economics & Management,\\nvol.5, 81-102, 1978.   Used in Belsley, Kuh & Welsch, 'Regression diagnostics\\n...', Wiley, 1980.   N.B. Various transformations are used in the table on\\npages 244-261 of the latter.\\n\\nThe Boston house-price data has been used in many machine learning papers that address regression\\nproblems.   \\n     \\n.. topic:: References\\n\\n   - Belsley, Kuh & Welsch, 'Regression diagnostics: Identifying Influential Data and Sources of Collinearity', Wiley, 1980. 244-261.\\n   - Quinlan,R. (1993). Combining Instance-Based and Model-Based Learning. In Proceedings on the Tenth International Conference of Machine Learning, 236-243, University of Massachusetts, Amherst. Morgan Kaufmann.\\n\",\n",
       " 'filename': 'C:\\\\Users\\\\rohit\\\\Anaconda3\\\\envs\\\\pysyft\\\\lib\\\\site-packages\\\\sklearn\\\\datasets\\\\data\\\\boston_house_prices.csv'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "boston_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['data', 'target', 'feature_names', 'DESCR', 'filename'])\n"
     ]
    }
   ],
   "source": [
    "print(boston.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".. _boston_dataset:\n",
      "\n",
      "Boston house prices dataset\n",
      "---------------------------\n",
      "\n",
      "**Data Set Characteristics:**  \n",
      "\n",
      "    :Number of Instances: 506 \n",
      "\n",
      "    :Number of Attributes: 13 numeric/categorical predictive. Median Value (attribute 14) is usually the target.\n",
      "\n",
      "    :Attribute Information (in order):\n",
      "        - CRIM     per capita crime rate by town\n",
      "        - ZN       proportion of residential land zoned for lots over 25,000 sq.ft.\n",
      "        - INDUS    proportion of non-retail business acres per town\n",
      "        - CHAS     Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)\n",
      "        - NOX      nitric oxides concentration (parts per 10 million)\n",
      "        - RM       average number of rooms per dwelling\n",
      "        - AGE      proportion of owner-occupied units built prior to 1940\n",
      "        - DIS      weighted distances to five Boston employment centres\n",
      "        - RAD      index of accessibility to radial highways\n",
      "        - TAX      full-value property-tax rate per $10,000\n",
      "        - PTRATIO  pupil-teacher ratio by town\n",
      "        - B        1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town\n",
      "        - LSTAT    % lower status of the population\n",
      "        - MEDV     Median value of owner-occupied homes in $1000's\n",
      "\n",
      "    :Missing Attribute Values: None\n",
      "\n",
      "    :Creator: Harrison, D. and Rubinfeld, D.L.\n",
      "\n",
      "This is a copy of UCI ML housing dataset.\n",
      "https://archive.ics.uci.edu/ml/machine-learning-databases/housing/\n",
      "\n",
      "\n",
      "This dataset was taken from the StatLib library which is maintained at Carnegie Mellon University.\n",
      "\n",
      "The Boston house-price data of Harrison, D. and Rubinfeld, D.L. 'Hedonic\n",
      "prices and the demand for clean air', J. Environ. Economics & Management,\n",
      "vol.5, 81-102, 1978.   Used in Belsley, Kuh & Welsch, 'Regression diagnostics\n",
      "...', Wiley, 1980.   N.B. Various transformations are used in the table on\n",
      "pages 244-261 of the latter.\n",
      "\n",
      "The Boston house-price data has been used in many machine learning papers that address regression\n",
      "problems.   \n",
      "     \n",
      ".. topic:: References\n",
      "\n",
      "   - Belsley, Kuh & Welsch, 'Regression diagnostics: Identifying Influential Data and Sources of Collinearity', Wiley, 1980. 244-261.\n",
      "   - Quinlan,R. (1993). Combining Instance-Based and Model-Based Learning. In Proceedings on the Tenth International Conference of Machine Learning, 236-243, University of Massachusetts, Amherst. Morgan Kaufmann.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(boston['DESCR'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00632</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2.31</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>6.575</td>\n",
       "      <td>65.2</td>\n",
       "      <td>4.0900</td>\n",
       "      <td>1.0</td>\n",
       "      <td>296.0</td>\n",
       "      <td>15.3</td>\n",
       "      <td>396.90</td>\n",
       "      <td>4.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.02731</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>6.421</td>\n",
       "      <td>78.9</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2.0</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>396.90</td>\n",
       "      <td>9.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.02729</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>7.185</td>\n",
       "      <td>61.1</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2.0</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>392.83</td>\n",
       "      <td>4.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.03237</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>6.998</td>\n",
       "      <td>45.8</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>394.63</td>\n",
       "      <td>2.94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.06905</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>7.147</td>\n",
       "      <td>54.2</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>396.90</td>\n",
       "      <td>5.33</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         0     1     2    3      4      5     6       7    8      9    10  \\\n",
       "0  0.00632  18.0  2.31  0.0  0.538  6.575  65.2  4.0900  1.0  296.0  15.3   \n",
       "1  0.02731   0.0  7.07  0.0  0.469  6.421  78.9  4.9671  2.0  242.0  17.8   \n",
       "2  0.02729   0.0  7.07  0.0  0.469  7.185  61.1  4.9671  2.0  242.0  17.8   \n",
       "3  0.03237   0.0  2.18  0.0  0.458  6.998  45.8  6.0622  3.0  222.0  18.7   \n",
       "4  0.06905   0.0  2.18  0.0  0.458  7.147  54.2  6.0622  3.0  222.0  18.7   \n",
       "\n",
       "       11    12  \n",
       "0  396.90  4.98  \n",
       "1  396.90  9.14  \n",
       "2  392.83  4.03  \n",
       "3  394.63  2.94  \n",
       "4  396.90  5.33  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "boston_df = pd.DataFrame(boston['data'] )\n",
    "boston_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CRIM</th>\n",
       "      <th>ZN</th>\n",
       "      <th>INDUS</th>\n",
       "      <th>CHAS</th>\n",
       "      <th>NOX</th>\n",
       "      <th>RM</th>\n",
       "      <th>AGE</th>\n",
       "      <th>DIS</th>\n",
       "      <th>RAD</th>\n",
       "      <th>TAX</th>\n",
       "      <th>PTRATIO</th>\n",
       "      <th>B</th>\n",
       "      <th>LSTAT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00632</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2.31</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>6.575</td>\n",
       "      <td>65.2</td>\n",
       "      <td>4.0900</td>\n",
       "      <td>1.0</td>\n",
       "      <td>296.0</td>\n",
       "      <td>15.3</td>\n",
       "      <td>396.90</td>\n",
       "      <td>4.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.02731</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>6.421</td>\n",
       "      <td>78.9</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2.0</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>396.90</td>\n",
       "      <td>9.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.02729</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>7.185</td>\n",
       "      <td>61.1</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2.0</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>392.83</td>\n",
       "      <td>4.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.03237</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>6.998</td>\n",
       "      <td>45.8</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>394.63</td>\n",
       "      <td>2.94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.06905</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>7.147</td>\n",
       "      <td>54.2</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>396.90</td>\n",
       "      <td>5.33</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      CRIM    ZN  INDUS  CHAS    NOX     RM   AGE     DIS  RAD    TAX  \\\n",
       "0  0.00632  18.0   2.31   0.0  0.538  6.575  65.2  4.0900  1.0  296.0   \n",
       "1  0.02731   0.0   7.07   0.0  0.469  6.421  78.9  4.9671  2.0  242.0   \n",
       "2  0.02729   0.0   7.07   0.0  0.469  7.185  61.1  4.9671  2.0  242.0   \n",
       "3  0.03237   0.0   2.18   0.0  0.458  6.998  45.8  6.0622  3.0  222.0   \n",
       "4  0.06905   0.0   2.18   0.0  0.458  7.147  54.2  6.0622  3.0  222.0   \n",
       "\n",
       "   PTRATIO       B  LSTAT  \n",
       "0     15.3  396.90   4.98  \n",
       "1     17.8  396.90   9.14  \n",
       "2     17.8  392.83   4.03  \n",
       "3     18.7  394.63   2.94  \n",
       "4     18.7  396.90   5.33  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "boston_df.columns = boston['feature_names']\n",
    "boston_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CRIM</th>\n",
       "      <th>ZN</th>\n",
       "      <th>INDUS</th>\n",
       "      <th>CHAS</th>\n",
       "      <th>NOX</th>\n",
       "      <th>RM</th>\n",
       "      <th>AGE</th>\n",
       "      <th>DIS</th>\n",
       "      <th>RAD</th>\n",
       "      <th>TAX</th>\n",
       "      <th>PTRATIO</th>\n",
       "      <th>B</th>\n",
       "      <th>LSTAT</th>\n",
       "      <th>PRICE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00632</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2.31</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>6.575</td>\n",
       "      <td>65.2</td>\n",
       "      <td>4.0900</td>\n",
       "      <td>1.0</td>\n",
       "      <td>296.0</td>\n",
       "      <td>15.3</td>\n",
       "      <td>396.90</td>\n",
       "      <td>4.98</td>\n",
       "      <td>24.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.02731</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>6.421</td>\n",
       "      <td>78.9</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2.0</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>396.90</td>\n",
       "      <td>9.14</td>\n",
       "      <td>21.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.02729</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>7.185</td>\n",
       "      <td>61.1</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2.0</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>392.83</td>\n",
       "      <td>4.03</td>\n",
       "      <td>34.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.03237</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>6.998</td>\n",
       "      <td>45.8</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>394.63</td>\n",
       "      <td>2.94</td>\n",
       "      <td>33.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.06905</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>7.147</td>\n",
       "      <td>54.2</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>396.90</td>\n",
       "      <td>5.33</td>\n",
       "      <td>36.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      CRIM    ZN  INDUS  CHAS    NOX     RM   AGE     DIS  RAD    TAX  \\\n",
       "0  0.00632  18.0   2.31   0.0  0.538  6.575  65.2  4.0900  1.0  296.0   \n",
       "1  0.02731   0.0   7.07   0.0  0.469  6.421  78.9  4.9671  2.0  242.0   \n",
       "2  0.02729   0.0   7.07   0.0  0.469  7.185  61.1  4.9671  2.0  242.0   \n",
       "3  0.03237   0.0   2.18   0.0  0.458  6.998  45.8  6.0622  3.0  222.0   \n",
       "4  0.06905   0.0   2.18   0.0  0.458  7.147  54.2  6.0622  3.0  222.0   \n",
       "\n",
       "   PTRATIO       B  LSTAT  PRICE  \n",
       "0     15.3  396.90   4.98   24.0  \n",
       "1     17.8  396.90   9.14   21.6  \n",
       "2     17.8  392.83   4.03   34.7  \n",
       "3     18.7  394.63   2.94   33.4  \n",
       "4     18.7  396.90   5.33   36.2  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "boston_df['PRICE']= boston['target']\n",
    "boston_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CRIM       0\n",
      "ZN         0\n",
      "INDUS      0\n",
      "CHAS       0\n",
      "NOX        0\n",
      "RM         0\n",
      "AGE        0\n",
      "DIS        0\n",
      "RAD        0\n",
      "TAX        0\n",
      "PTRATIO    0\n",
      "B          0\n",
      "LSTAT      0\n",
      "PRICE      0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "#check for missing values\n",
    "print(np.sum(np.isnan(boston_df)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'seaborn'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-c87d9f858beb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mseaborn\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0msns\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0msns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdistplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mboston_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'PRICE'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m;\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'seaborn'"
     ]
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "sns.distplot(boston_df['PRICE']);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(354, 13) (152, 13) (354,) (152,)\n"
     ]
    }
   ],
   "source": [
    "#This will throw and error at import if haven't upgraded. \n",
    "# from sklearn.cross_validation  import train_test_split  \n",
    "from sklearn.model_selection  import train_test_split\n",
    "#y is the dependent variable.\n",
    "y = boston_df['PRICE']\n",
    "#As we know, iloc is used to slice the array by index number. Here this is the matrix of \n",
    "#independent variables.\n",
    "X = boston_df.iloc[:,0:13]\n",
    "\n",
    "# Split the data into a training set and a test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\n",
    "\n",
    "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define training hyperprameters.\n",
    "batch_size = 50\n",
    "num_epochs = 200\n",
    "learning_rate = 0.01\n",
    "size_hidden= 100\n",
    "\n",
    "#Calculate some other hyperparameters based on data.  \n",
    "batch_no = len(X_train) // batch_size  #batches\n",
    "cols=X_train.shape[1] #Number of columns in input matrix\n",
    "n_output=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create the model\n",
    "#device = th.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "# Assume that we are on a CUDA machine, then this should print a CUDA device:\n",
    "#print(\"Executing the model on :\",device)\n",
    "class Net(th.nn.Module):\n",
    "    def __init__(self, n_feature, size_hidden, n_output):\n",
    "        super(Net, self).__init__()\n",
    "        self.hidden = th.nn.Linear(cols, size_hidden)   # hidden layer\n",
    "        self.predict = th.nn.Linear(size_hidden, n_output)   # output layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.hidden(x))      # activation function for hidden layer\n",
    "        x = self.predict(x)             # linear output\n",
    "        return x\n",
    "net = Net(cols, size_hidden, n_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rohit\\Anaconda3\\envs\\pysyft\\lib\\site-packages\\torch\\nn\\_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    }
   ],
   "source": [
    "#Adam is a specific flavor of gradient decent which is typically better\n",
    "optimizer = th.optim.Adam(net.parameters(), lr=learning_rate)\n",
    "#optimizer = torch.optim.SGD(net.parameters(), lr=0.2)\n",
    "criterion = th.nn.MSELoss(size_average=False)  # this is for regression mean squared loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train=X_train.values\n",
    "y_train=y_train.values\n",
    "X_test=X_test.values\n",
    "y_test=y_test.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 loss:  181347.96728515625\n",
      "Epoch 2 loss:  90810.67846679688\n",
      "Epoch 3 loss:  46022.29150390625\n",
      "Epoch 4 loss:  37215.123046875\n",
      "Epoch 5 loss:  26084.189331054688\n",
      "Epoch 6 loss:  20339.444580078125\n",
      "Epoch 7 loss:  20565.439453125\n",
      "Epoch 8 loss:  19744.40380859375\n",
      "Epoch 9 loss:  17544.4365234375\n",
      "Epoch 10 loss:  17945.580078125\n",
      "Epoch 11 loss:  18865.500732421875\n",
      "Epoch 12 loss:  16611.59521484375\n",
      "Epoch 13 loss:  16788.924682617188\n",
      "Epoch 14 loss:  15975.029907226562\n",
      "Epoch 15 loss:  15704.378295898438\n",
      "Epoch 16 loss:  15147.650024414062\n",
      "Epoch 17 loss:  15558.355102539062\n",
      "Epoch 18 loss:  15062.959594726562\n",
      "Epoch 19 loss:  14561.671875\n",
      "Epoch 20 loss:  14518.320068359375\n",
      "Epoch 21 loss:  15115.710205078125\n",
      "Epoch 22 loss:  13458.443969726562\n",
      "Epoch 23 loss:  13560.556884765625\n",
      "Epoch 24 loss:  13829.452758789062\n",
      "Epoch 25 loss:  12678.4755859375\n",
      "Epoch 26 loss:  12619.242431640625\n",
      "Epoch 27 loss:  13844.245239257812\n",
      "Epoch 28 loss:  13400.854675292969\n",
      "Epoch 29 loss:  12275.756958007812\n",
      "Epoch 30 loss:  13204.287719726562\n",
      "Epoch 31 loss:  13631.402709960938\n",
      "Epoch 32 loss:  13384.078247070312\n",
      "Epoch 33 loss:  11591.736999511719\n",
      "Epoch 34 loss:  11474.42626953125\n",
      "Epoch 35 loss:  11561.475769042969\n",
      "Epoch 36 loss:  10862.089721679688\n",
      "Epoch 37 loss:  11580.825561523438\n",
      "Epoch 38 loss:  10706.11328125\n",
      "Epoch 39 loss:  10544.743041992188\n",
      "Epoch 40 loss:  11278.428955078125\n",
      "Epoch 41 loss:  12349.856323242188\n",
      "Epoch 42 loss:  11563.952453613281\n",
      "Epoch 43 loss:  11630.356872558594\n",
      "Epoch 44 loss:  11348.988586425781\n",
      "Epoch 45 loss:  10812.297790527344\n",
      "Epoch 46 loss:  11352.854858398438\n",
      "Epoch 47 loss:  11875.106323242188\n",
      "Epoch 48 loss:  10718.345581054688\n",
      "Epoch 49 loss:  10009.969116210938\n",
      "Epoch 50 loss:  9975.375915527344\n",
      "Epoch 51 loss:  11405.296264648438\n",
      "Epoch 52 loss:  11080.794555664062\n",
      "Epoch 53 loss:  9484.526977539062\n",
      "Epoch 54 loss:  9812.098876953125\n",
      "Epoch 55 loss:  9785.095092773438\n",
      "Epoch 56 loss:  9340.877319335938\n",
      "Epoch 57 loss:  9321.763427734375\n",
      "Epoch 58 loss:  9588.227111816406\n",
      "Epoch 59 loss:  9692.633178710938\n",
      "Epoch 60 loss:  9932.395568847656\n",
      "Epoch 61 loss:  8704.861511230469\n",
      "Epoch 62 loss:  8763.645935058594\n",
      "Epoch 63 loss:  10711.622802734375\n",
      "Epoch 64 loss:  10128.491760253906\n",
      "Epoch 65 loss:  13792.58349609375\n",
      "Epoch 66 loss:  18516.872802734375\n",
      "Epoch 67 loss:  11492.938354492188\n",
      "Epoch 68 loss:  9857.83154296875\n",
      "Epoch 69 loss:  10160.592407226562\n",
      "Epoch 70 loss:  9628.606079101562\n",
      "Epoch 71 loss:  9310.32421875\n",
      "Epoch 72 loss:  9533.630432128906\n",
      "Epoch 73 loss:  10049.826293945312\n",
      "Epoch 74 loss:  9976.217956542969\n",
      "Epoch 75 loss:  9465.897827148438\n",
      "Epoch 76 loss:  9852.3740234375\n",
      "Epoch 77 loss:  9182.651672363281\n",
      "Epoch 78 loss:  9671.845825195312\n",
      "Epoch 79 loss:  10086.069885253906\n",
      "Epoch 80 loss:  9726.040283203125\n",
      "Epoch 81 loss:  7883.3433837890625\n",
      "Epoch 82 loss:  8744.464050292969\n",
      "Epoch 83 loss:  8841.502014160156\n",
      "Epoch 84 loss:  9237.121398925781\n",
      "Epoch 85 loss:  9002.267333984375\n",
      "Epoch 86 loss:  9384.688781738281\n",
      "Epoch 87 loss:  8210.454162597656\n",
      "Epoch 88 loss:  8119.141540527344\n",
      "Epoch 89 loss:  10542.371826171875\n",
      "Epoch 90 loss:  10169.154907226562\n",
      "Epoch 91 loss:  8063.320556640625\n",
      "Epoch 92 loss:  10994.4853515625\n",
      "Epoch 93 loss:  7524.2760009765625\n",
      "Epoch 94 loss:  7657.609161376953\n",
      "Epoch 95 loss:  8616.356567382812\n",
      "Epoch 96 loss:  9204.472412109375\n",
      "Epoch 97 loss:  10804.408813476562\n",
      "Epoch 98 loss:  7922.685974121094\n",
      "Epoch 99 loss:  8124.112487792969\n",
      "Epoch 100 loss:  7978.666198730469\n",
      "Epoch 101 loss:  8887.0107421875\n",
      "Epoch 102 loss:  8237.85546875\n",
      "Epoch 103 loss:  7889.421691894531\n",
      "Epoch 104 loss:  7785.40185546875\n",
      "Epoch 105 loss:  8542.964477539062\n",
      "Epoch 106 loss:  7521.586608886719\n",
      "Epoch 107 loss:  7966.949462890625\n",
      "Epoch 108 loss:  7125.441223144531\n",
      "Epoch 109 loss:  7224.23095703125\n",
      "Epoch 110 loss:  7355.86328125\n",
      "Epoch 111 loss:  6955.5740966796875\n",
      "Epoch 112 loss:  8495.55517578125\n",
      "Epoch 113 loss:  12929.2294921875\n",
      "Epoch 114 loss:  10456.637451171875\n",
      "Epoch 115 loss:  8438.677978515625\n",
      "Epoch 116 loss:  7525.3536376953125\n",
      "Epoch 117 loss:  6889.2083740234375\n",
      "Epoch 118 loss:  7805.721740722656\n",
      "Epoch 119 loss:  8527.136779785156\n",
      "Epoch 120 loss:  9644.782653808594\n",
      "Epoch 121 loss:  11329.71142578125\n",
      "Epoch 122 loss:  8451.484252929688\n",
      "Epoch 123 loss:  7991.147155761719\n",
      "Epoch 124 loss:  9906.047973632812\n",
      "Epoch 125 loss:  9632.551879882812\n",
      "Epoch 126 loss:  9404.585205078125\n",
      "Epoch 127 loss:  10771.530883789062\n",
      "Epoch 128 loss:  12750.360290527344\n",
      "Epoch 129 loss:  10633.317565917969\n",
      "Epoch 130 loss:  9917.696716308594\n",
      "Epoch 131 loss:  7736.726867675781\n",
      "Epoch 132 loss:  10030.661010742188\n",
      "Epoch 133 loss:  9268.919677734375\n",
      "Epoch 134 loss:  7104.483947753906\n",
      "Epoch 135 loss:  6355.041564941406\n",
      "Epoch 136 loss:  6535.715576171875\n",
      "Epoch 137 loss:  7753.071838378906\n",
      "Epoch 138 loss:  7471.926330566406\n",
      "Epoch 139 loss:  6127.433532714844\n",
      "Epoch 140 loss:  6282.13427734375\n",
      "Epoch 141 loss:  7178.485656738281\n",
      "Epoch 142 loss:  6040.237121582031\n",
      "Epoch 143 loss:  6596.255126953125\n",
      "Epoch 144 loss:  8301.82763671875\n",
      "Epoch 145 loss:  7208.361267089844\n",
      "Epoch 146 loss:  7305.785705566406\n",
      "Epoch 147 loss:  7197.1942138671875\n",
      "Epoch 148 loss:  7408.0633544921875\n",
      "Epoch 149 loss:  7646.51318359375\n",
      "Epoch 150 loss:  7475.389404296875\n",
      "Epoch 151 loss:  7915.0899658203125\n",
      "Epoch 152 loss:  6434.342468261719\n",
      "Epoch 153 loss:  6209.981140136719\n",
      "Epoch 154 loss:  6599.44677734375\n",
      "Epoch 155 loss:  6557.7327880859375\n",
      "Epoch 156 loss:  6963.9576416015625\n",
      "Epoch 157 loss:  7405.050842285156\n",
      "Epoch 158 loss:  6089.467041015625\n",
      "Epoch 159 loss:  6061.448486328125\n",
      "Epoch 160 loss:  6020.251770019531\n",
      "Epoch 161 loss:  6167.676086425781\n",
      "Epoch 162 loss:  5819.427886962891\n",
      "Epoch 163 loss:  5798.949768066406\n",
      "Epoch 164 loss:  6451.825988769531\n",
      "Epoch 165 loss:  5909.4349365234375\n",
      "Epoch 166 loss:  5498.952819824219\n",
      "Epoch 167 loss:  6287.428405761719\n",
      "Epoch 168 loss:  5968.709655761719\n",
      "Epoch 169 loss:  5271.5738525390625\n",
      "Epoch 170 loss:  6505.5948486328125\n",
      "Epoch 171 loss:  6518.373046875\n",
      "Epoch 172 loss:  6161.784851074219\n",
      "Epoch 173 loss:  5549.933410644531\n",
      "Epoch 174 loss:  7010.601623535156\n",
      "Epoch 175 loss:  6968.753326416016\n",
      "Epoch 176 loss:  6406.2435302734375\n",
      "Epoch 177 loss:  6738.924377441406\n",
      "Epoch 178 loss:  6376.004791259766\n",
      "Epoch 179 loss:  5488.190124511719\n",
      "Epoch 180 loss:  5843.838562011719\n",
      "Epoch 181 loss:  5800.9849853515625\n",
      "Epoch 182 loss:  5886.3839111328125\n",
      "Epoch 183 loss:  7403.35400390625\n",
      "Epoch 184 loss:  6781.023406982422\n",
      "Epoch 185 loss:  8177.938232421875\n",
      "Epoch 186 loss:  9587.540893554688\n",
      "Epoch 187 loss:  6641.982177734375\n",
      "Epoch 188 loss:  5692.377380371094\n",
      "Epoch 189 loss:  5573.528991699219\n",
      "Epoch 190 loss:  5464.42236328125\n",
      "Epoch 191 loss:  7687.242431640625\n",
      "Epoch 192 loss:  7907.233093261719\n",
      "Epoch 193 loss:  5898.022064208984\n",
      "Epoch 194 loss:  5180.5791015625\n",
      "Epoch 195 loss:  5310.472747802734\n",
      "Epoch 196 loss:  5031.967529296875\n",
      "Epoch 197 loss:  5744.857940673828\n",
      "Epoch 198 loss:  5030.807708740234\n",
      "Epoch 199 loss:  4527.153961181641\n",
      "Epoch 200 loss:  5390.755676269531\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils import shuffle\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "running_loss = 0.0\n",
    "for epoch in range(num_epochs):\n",
    "    #Shuffle just mixes up the dataset between epocs\n",
    "    X_train, y_train = shuffle(X_train, y_train)\n",
    "    # Mini batch learning\n",
    "    for i in range(batch_no):\n",
    "        start = i * batch_size\n",
    "        end = start + batch_size\n",
    "        inputs = Variable(th.FloatTensor(X_train[start:end]))\n",
    "        labels = Variable(th.FloatTensor(y_train[start:end]))\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(inputs)\n",
    "        #print(\"outputs\",outputs)\n",
    "        #print(\"outputs\",outputs,outputs.shape,\"labels\",labels, labels.shape)\n",
    "        loss = criterion(outputs, th.unsqueeze(labels,dim=1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "    print('Epoch {}'.format(epoch+1), \"loss: \",running_loss)\n",
    "    running_loss = 0.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "encrypted_model = net.fix_precision().share(alice, bob, crypto_provider=secure_worker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parameter containing:\n",
       " Parameter>FixedPrecisionTensor>[AdditiveSharingTensor]\n",
       " \t-> [PointerTensor | me:76957471575 -> alice:83802234493]\n",
       " \t-> [PointerTensor | me:39388983586 -> bob:98134551698]\n",
       " \t*crypto provider: secure_worker*, Parameter containing:\n",
       " Parameter>FixedPrecisionTensor>[AdditiveSharingTensor]\n",
       " \t-> [PointerTensor | me:30411124338 -> alice:21709651841]\n",
       " \t-> [PointerTensor | me:64410425846 -> bob:93094355407]\n",
       " \t*crypto provider: secure_worker*, Parameter containing:\n",
       " Parameter>FixedPrecisionTensor>[AdditiveSharingTensor]\n",
       " \t-> [PointerTensor | me:33477402082 -> alice:86757495180]\n",
       " \t-> [PointerTensor | me:32770708032 -> bob:25604185543]\n",
       " \t*crypto provider: secure_worker*, Parameter containing:\n",
       " Parameter>FixedPrecisionTensor>[AdditiveSharingTensor]\n",
       " \t-> [PointerTensor | me:95958893495 -> alice:82920306388]\n",
       " \t-> [PointerTensor | me:33503270688 -> bob:32240710828]\n",
       " \t*crypto provider: secure_worker*]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(encrypted_model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'fix_precision'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-38-d71d31406630>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mencrypted_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfix_precision\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshare\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0malice\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbob\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcrypto_provider\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msecure_worker\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'fix_precision'"
     ]
    }
   ],
   "source": [
    "encrypted_data = X_test.fix_precision().share(alice, bob, crypto_provider=secure_worker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = th.tensor(X_test, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "encrypted_data = data.fix_precision().share(alice, bob, crypto_provider=secure_worker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Wrapper)>FixedPrecisionTensor>[AdditiveSharingTensor]\n",
       "\t-> [PointerTensor | me:61399528615 -> alice:17367867413]\n",
       "\t-> [PointerTensor | me:51625165895 -> bob:84199009562]\n",
       "\t*crypto provider: secure_worker*"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encrypted_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "encrypted_prediction=encrypted_model(encrypted_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[24.2890],\n",
       "        [23.2910],\n",
       "        [26.7490],\n",
       "        [ 9.9270],\n",
       "        [20.0820],\n",
       "        [20.8280],\n",
       "        [20.0370],\n",
       "        [21.8340],\n",
       "        [17.5470],\n",
       "        [14.1400],\n",
       "        [14.3690],\n",
       "        [16.1580],\n",
       "        [15.7290],\n",
       "        [ 7.0570],\n",
       "        [44.0140],\n",
       "        [30.1170],\n",
       "        [21.1130],\n",
       "        [37.0080],\n",
       "        [33.9160],\n",
       "        [22.9210],\n",
       "        [23.9500],\n",
       "        [20.3760],\n",
       "        [18.8690],\n",
       "        [25.3900],\n",
       "        [21.4690],\n",
       "        [15.9710],\n",
       "        [17.2060],\n",
       "        [17.1510],\n",
       "        [34.7500],\n",
       "        [17.5310],\n",
       "        [13.4280],\n",
       "        [16.8900],\n",
       "        [18.0680],\n",
       "        [20.1150],\n",
       "        [25.1360],\n",
       "        [19.9980],\n",
       "        [ 8.7920],\n",
       "        [26.0180],\n",
       "        [13.6640],\n",
       "        [12.5620],\n",
       "        [26.3120],\n",
       "        [19.8650],\n",
       "        [19.3420],\n",
       "        [13.2510],\n",
       "        [21.6920],\n",
       "        [22.9060],\n",
       "        [19.6490],\n",
       "        [20.7420],\n",
       "        [16.7840],\n",
       "        [22.9050],\n",
       "        [17.4690],\n",
       "        [17.7290],\n",
       "        [21.9130],\n",
       "        [32.1990],\n",
       "        [14.9350],\n",
       "        [20.9400],\n",
       "        [18.6090],\n",
       "        [15.9350],\n",
       "        [18.3170],\n",
       "        [19.6420],\n",
       "        [23.3990],\n",
       "        [21.2290],\n",
       "        [31.4280],\n",
       "        [36.1570],\n",
       "        [15.4710],\n",
       "        [34.1730],\n",
       "        [15.6060],\n",
       "        [17.9960],\n",
       "        [15.9480],\n",
       "        [22.9820],\n",
       "        [19.1200],\n",
       "        [22.7840],\n",
       "        [29.3160],\n",
       "        [28.8860],\n",
       "        [26.4180],\n",
       "        [ 5.1680],\n",
       "        [36.6940],\n",
       "        [22.1090],\n",
       "        [22.6870],\n",
       "        [20.1870],\n",
       "        [23.7420],\n",
       "        [19.6510],\n",
       "        [18.6780],\n",
       "        [37.5790],\n",
       "        [37.6280],\n",
       "        [25.2300],\n",
       "        [20.5900],\n",
       "        [ 9.9140],\n",
       "        [27.5830],\n",
       "        [12.9840],\n",
       "        [21.4530],\n",
       "        [12.9860],\n",
       "        [25.0050],\n",
       "        [26.9310],\n",
       "        [20.4230],\n",
       "        [20.9120],\n",
       "        [ 4.3230],\n",
       "        [27.8830],\n",
       "        [14.3380],\n",
       "        [17.6360],\n",
       "        [23.5160],\n",
       "        [20.5900],\n",
       "        [27.7560],\n",
       "        [21.6800],\n",
       "        [22.2620],\n",
       "        [21.8300],\n",
       "        [ 8.9740],\n",
       "        [16.1380],\n",
       "        [20.2610],\n",
       "        [24.7390],\n",
       "        [30.8510],\n",
       "        [10.9350],\n",
       "        [18.2490],\n",
       "        [13.4180],\n",
       "        [15.8030],\n",
       "        [21.4270],\n",
       "        [ 7.9810],\n",
       "        [20.9700],\n",
       "        [ 9.4720],\n",
       "        [50.8250],\n",
       "        [32.3180],\n",
       "        [11.2070],\n",
       "        [17.8050],\n",
       "        [21.4570],\n",
       "        [21.9460],\n",
       "        [20.2730],\n",
       "        [37.1880],\n",
       "        [14.3530],\n",
       "        [20.7190],\n",
       "        [29.7750],\n",
       "        [15.3510],\n",
       "        [13.1840],\n",
       "        [15.2430],\n",
       "        [17.7990],\n",
       "        [11.0270],\n",
       "        [35.1130],\n",
       "        [22.6420],\n",
       "        [20.2160],\n",
       "        [24.8500],\n",
       "        [13.1350],\n",
       "        [ 9.9120],\n",
       "        [20.9830],\n",
       "        [38.4560],\n",
       "        [25.9330],\n",
       "        [26.6850],\n",
       "        [15.2100],\n",
       "        [32.9050],\n",
       "        [30.1010],\n",
       "        [10.5440],\n",
       "        [ 7.6540],\n",
       "        [27.5530],\n",
       "        [27.7050]])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encrypted_prediction.get().float_precision()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
